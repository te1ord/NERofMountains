{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "sys.path.append(parent_directory)\n",
    "\n",
    "from utils.constants import init_datagen_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MOUNTAINS_NAMES_PATH': '../dataset/synthetic/Mountain.csv',\n",
       " 'SAVE_DATASET_PATH': '../dataset/synthetic/mountains_only_synthetic.txt',\n",
       " 'SAVE_PROCESSED_PATH': '../../dataset/synthetic/synthetic_processed.json',\n",
       " 'GENERATOR_MODEL': 'meta/llama-3.1-405b-instruct',\n",
       " 'MIN_SAMPLES': 0,\n",
       " 'MAX_SAMPLES': 3,\n",
       " 'TEMPERATURE': 0.9,\n",
       " 'WNUT_DATA_PATH': '../../dataset/wnut16/wnut 16.txt.conll',\n",
       " 'PROCESSED_WNUT_PATH': '../../dataset/wnut16/wnut_processed.csv',\n",
       " 'FEW-NERD_BALANCED_TRAIN_PATH': '../../dataset/few-nerd/train',\n",
       " 'FEW-NERD_BALANCED_VAL_PATH': '../../dataset/few-nerd/val',\n",
       " 'FEW-NERD_BALANCED_TEST_PATH': '../../dataset/few-nerd/test',\n",
       " 'FINAL_DATASET_PATH': '../../dataset/resulting_dataset'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAGEN_CONFIG = init_datagen_config('../configs/datagen.yaml')\n",
    "DATAGEN_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", DATAGEN_CONFIG['SAVE_DATASET_PATH']), 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "def preprocess_for_ner(dataset):\n",
    "    ner_data = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        mountain_name = entry[\"mountain\"]  \n",
    "        sentences = entry[\"sentences\"]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = re.findall(r'\\w+|[^\\w\\s]', sentence, re.UNICODE)\n",
    "            labels = [0] * len(tokens)  # Start with all '0' (Outside)\n",
    "\n",
    "            # Regex to find mountain names and label them\n",
    "            mountain_regex = re.escape(mountain_name)\n",
    "            for match in re.finditer(mountain_regex, sentence):\n",
    "                start, end = match.span()\n",
    "\n",
    "                start_word = len(re.findall(r'\\w+|[^\\w\\s]', sentence[:start], re.UNICODE))\n",
    "                end_word = len(re.findall(r'\\w+|[^\\w\\s]', sentence[:end], re.UNICODE))\n",
    "\n",
    "                # Update labels using the few-nerd convention\n",
    "                labels[start_word] = 1  # B-Mountain\n",
    "                for i in range(start_word + 1, end_word):\n",
    "                    labels[i] = 2  # I-Mountain\n",
    "\n",
    "            ner_data.append({\n",
    "                \"sentence\": sentence,\n",
    "                \"tokens\": tokens,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'Mount Everest, the highest peak in the world, was formed approximately 60 million years ago when India collided with Eurasia.', 'tokens': ['Mount', 'Everest', ',', 'the', 'highest', 'peak', 'in', 'the', 'world', ',', 'was', 'formed', 'approximately', '60', 'million', 'years', 'ago', 'when', 'India', 'collided', 'with', 'Eurasia', '.'], 'labels': [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'sentence': 'Climbing K2, the second-highest peak in the world, is a formidable challenge for even the most seasoned mountaineers, requiring meticulous planning and physical endurance.', 'tokens': ['Climbing', 'K2', ',', 'the', 'second', '-', 'highest', 'peak', 'in', 'the', 'world', ',', 'is', 'a', 'formidable', 'challenge', 'for', 'even', 'the', 'most', 'seasoned', 'mountaineers', ',', 'requiring', 'meticulous', 'planning', 'and', 'physical', 'endurance', '.'], 'labels': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'sentence': \"Located on the Pakistan-China border, K2's snow-capped peak stands out starkly against the rugged landscape of the Karakoram range.\", 'tokens': ['Located', 'on', 'the', 'Pakistan', '-', 'China', 'border', ',', 'K2', \"'\", 's', 'snow', '-', 'capped', 'peak', 'stands', 'out', 'starkly', 'against', 'the', 'rugged', 'landscape', 'of', 'the', 'Karakoram', 'range', '.'], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'sentence': 'Despite several attempts, I have yet to successfully summit K2, but the allure of its majestic beauty and the promise of adventure keep drawing me back to this mighty mountain.', 'tokens': ['Despite', 'several', 'attempts', ',', 'I', 'have', 'yet', 'to', 'successfully', 'summit', 'K2', ',', 'but', 'the', 'allure', 'of', 'its', 'majestic', 'beauty', 'and', 'the', 'promise', 'of', 'adventure', 'keep', 'drawing', 'me', 'back', 'to', 'this', 'mighty', 'mountain', '.'], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'sentence': 'As I stood at the base of Makalu, the fifth-highest mountain in the world, I felt an overwhelming sense of awe at the towering giant that loomed above me, its snow-capped peak glistening in the sunlight.', 'tokens': ['As', 'I', 'stood', 'at', 'the', 'base', 'of', 'Makalu', ',', 'the', 'fifth', '-', 'highest', 'mountain', 'in', 'the', 'world', ',', 'I', 'felt', 'an', 'overwhelming', 'sense', 'of', 'awe', 'at', 'the', 'towering', 'giant', 'that', 'loomed', 'above', 'me', ',', 'its', 'snow', '-', 'capped', 'peak', 'glistening', 'in', 'the', 'sunlight', '.'], 'labels': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "ner_formatted_data = preprocess_for_ner(dataset)\n",
    "\n",
    "with open(DATAGEN_CONFIG['SAVE_PROCESSED_PATH'], 'w') as f:\n",
    "    json.dump(ner_formatted_data, f, indent=2)\n",
    "\n",
    "iter=0\n",
    "for entry in ner_formatted_data:\n",
    "    print(entry)\n",
    "    iter +=1\n",
    "    if iter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 synthetic generated sentences\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(ner_formatted_data)} synthetic generated sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Mount Everest, the highest peak in the world, was formed approximately 60 million years ago when India collided with Eurasia.',\n",
       " 'tokens': ['Mount',\n",
       "  'Everest',\n",
       "  ',',\n",
       "  'the',\n",
       "  'highest',\n",
       "  'peak',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  ',',\n",
       "  'was',\n",
       "  'formed',\n",
       "  'approximately',\n",
       "  '60',\n",
       "  'million',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'when',\n",
       "  'India',\n",
       "  'collided',\n",
       "  'with',\n",
       "  'Eurasia',\n",
       "  '.'],\n",
       " 'labels': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_formatted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(ner_formatted_data[0]['labels'])==len(ner_formatted_data[0]['tokens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of domain datasets for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WNUT 16\n",
    "\n",
    "WNUT 2016 Dataset is annotated with 10 fine-grained NER categories: person, geo-location, company, facility, product,music artist, movie, sports team, tv show and other. Dataset was extracted from tweets and is structured in CoNLL format., in English language. Containing 5,63 in Text file format.\n",
    "https://autonlp.ai/datasets/wnut-2016\n",
    "\n",
    "#### Hope to extract some info about mountains from this dataset but it can be helpful even without vast amount of data related to our problem - can use it for balancing our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATAGEN_CONFIG['WNUT_DATA_PATH'], 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Parse the dataset into tokens and labels\n",
    "lines = data.split('\\n')\n",
    "tokens = []\n",
    "labels = []\n",
    "sentence_tokens = []\n",
    "sentence_labels = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.strip():\n",
    "        token, label = line.split('\\t')\n",
    "        sentence_tokens.append(token)\n",
    "        sentence_labels.append(label)\n",
    "    else:\n",
    "        if sentence_tokens:\n",
    "            tokens.append(sentence_tokens)\n",
    "            labels.append(sentence_labels)\n",
    "            sentence_tokens = []\n",
    "            sentence_labels = []\n",
    "\n",
    "# Ensure the last sentence is added\n",
    "if sentence_tokens:\n",
    "    tokens.append(sentence_tokens)\n",
    "    labels.append(sentence_labels)\n",
    "\n",
    "data_dict = {'tokens': tokens, 'labels': labels}\n",
    "data_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ls_n',\n",
       " 'perhaps',\n",
       " ',',\n",
       " 'but',\n",
       " 'folks',\n",
       " 'may',\n",
       " 'find',\n",
       " 'something',\n",
       " 'in',\n",
       " 'the',\n",
       " 'gallery',\n",
       " 'that',\n",
       " 'is',\n",
       " 'helpful',\n",
       " 'in',\n",
       " 'their',\n",
       " 'day-to-day',\n",
       " 'work',\n",
       " 'as',\n",
       " 'well',\n",
       " '.',\n",
       " 'Even',\n",
       " 'just',\n",
       " 'to',\n",
       " 'use',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[3]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@SammieLynnsMom, @tg10781, they, will, be, al...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Made, it, back, home, to, GA, ., It, sucks, n...</td>\n",
       "      <td>[O, O, O, O, O, B-geo-loc, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[', Breaking, Dawn, ', Returns, to, Vancouver,...</td>\n",
       "      <td>[O, B-movie, I-movie, O, O, O, B-geo-loc, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[@ls_n, perhaps, ,, but, folks, may, find, som...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[@Carr0t, aye, been, tonight, -, excellent]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [@SammieLynnsMom, @tg10781, they, will, be, al...   \n",
       "1  [Made, it, back, home, to, GA, ., It, sucks, n...   \n",
       "2  [', Breaking, Dawn, ', Returns, to, Vancouver,...   \n",
       "3  [@ls_n, perhaps, ,, but, folks, may, find, som...   \n",
       "4        [@Carr0t, aye, been, tonight, -, excellent]   \n",
       "\n",
       "                                              labels  \n",
       "0               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1  [O, O, O, O, O, B-geo-loc, O, O, O, O, O, O, O...  \n",
       "2  [O, B-movie, I-movie, O, O, O, B-geo-loc, O, O...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4                                 [O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the row that included null column 0\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.dropna()\n",
    "\n",
    "any_null_count = data_df.isnull().any(axis=1).sum()\n",
    "print(f'Count the row that included null column', any_null_count)\n",
    "\n",
    "def is_english(sentence):\n",
    "    return re.match(r'^[a-zA-Z0-9\\s,.\\'!?-]+$', ' '.join(sentence))\n",
    "\n",
    "# Filter out non-English sentences\n",
    "data_df = data_df[data_df['tokens'].apply(lambda x: bool(is_english(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags: {'I-product', 'B-sportsteam', 'I-person', 'B-movie', 'I-sportsteam', 'B-other', 'I-facility', 'B-product', 'B-company', 'B-tvshow', 'B-person', 'B-geo-loc', 'I-other', 'B-facility', 'B-musicartist', 'I-tvshow', 'I-movie', 'I-company', 'I-musicartist', 'I-geo-loc', 'O'}\n"
     ]
    }
   ],
   "source": [
    "# unique tags from the dataset\n",
    "unique_tags = set()\n",
    "\n",
    "for label_list in labels:\n",
    "    for label in label_list:\n",
    "        unique_tags.add(label)\n",
    "\n",
    "print(\"Unique tags:\", unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [tokens, labels, contains_mountain]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# rows that contain 'B-geo-loc' \n",
    "geo_df = data_df[data_df['labels'].apply(lambda x: 'B-geo-loc' in x)]\n",
    "geo_df.shape\n",
    "\n",
    "mountains_df = pd.read_csv(os.path.join('..', DATAGEN_CONFIG['MOUNTAINS_NAMES_PATH']))  \n",
    "mountain_names = mountains_df['Mountain'].tolist()  \n",
    "\n",
    "mountain_names = [name.lower() for name in mountain_names]\n",
    "def detect_mountains(tokens, mountain_names):\n",
    "    tokens_lower = [token.lower() for token in tokens]  # convert tokens to lowercase for comparison\n",
    "    return any(mountain in tokens_lower for mountain in mountain_names)\n",
    "\n",
    "nomountain = geo_df.copy()\n",
    "nomountain['contains_mountain'] = geo_df['tokens'].apply(lambda x: detect_mountains(x, mountain_names))\n",
    "\n",
    "mountain_rows = nomountain[nomountain['contains_mountain'] == True]\n",
    "print(mountain_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not have the entries about related to mountains let's just sample 196 samples without mountains to balance synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special symbols as this data is parsed from twitter, so may contain unuseful for language understanding info\n",
    "# Remain commas\n",
    "\n",
    "def preprocess_tokens(tokens):\n",
    "    # Join tokens into a single string for processing\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters and punctuation except commas\n",
    "    text = re.sub(r'[^a-zA-Z0-9,\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Split the processed text back into tokens\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>[before, the, season, even, starts, i, will, n...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>[Have, a, feeling, my, phone, bill, will, be, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>[looks, like, its, my, byzabedtime, hope, to, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>[Spent, all, of, last, night, puking, ,, and, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>[Im, trying, to, figure, out, if, I, wanna, do...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>[Tonight, Homemade, ice, cream, flights, try, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>[Love, me, when, I, least, deserve, it, ,, bec...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[he, likes, prince, ,, paul, simon, ,, and, od...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>[my, past, was, the, reason, i, tried, to, kil...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>[today, just, doesnt, feel, like, a, Friday]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "1093  [before, the, season, even, starts, i, will, n...   \n",
       "2373  [Have, a, feeling, my, phone, bill, will, be, ...   \n",
       "2004  [looks, like, its, my, byzabedtime, hope, to, ...   \n",
       "427   [Spent, all, of, last, night, puking, ,, and, ...   \n",
       "2324  [Im, trying, to, figure, out, if, I, wanna, do...   \n",
       "...                                                 ...   \n",
       "614   [Tonight, Homemade, ice, cream, flights, try, ...   \n",
       "2243  [Love, me, when, I, least, deserve, it, ,, bec...   \n",
       "31    [he, likes, prince, ,, paul, simon, ,, and, od...   \n",
       "2308  [my, past, was, the, reason, i, tried, to, kil...   \n",
       "78         [today, just, doesnt, feel, like, a, Friday]   \n",
       "\n",
       "                                                 labels  \n",
       "1093   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2373                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2004            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "427   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2324                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                                 ...  \n",
       "614       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2243  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "31     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2308  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "78                                [0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[196 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 196 examples\n",
    "sampled_mountain_rows = data_df.sample(n=196, random_state=42)\n",
    "sampled_mountain_rows['tokens'] = sampled_mountain_rows['tokens'].apply(preprocess_tokens)\n",
    "sampled_mountain_rows['labels'] = [[0] * len(tokens) for tokens in sampled_mountain_rows['tokens']]\n",
    "sampled_mountain_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_mountain_rows.iloc[0]['tokens']) == len(sampled_mountain_rows.iloc[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['before',\n",
       " 'the',\n",
       " 'season',\n",
       " 'even',\n",
       " 'starts',\n",
       " 'i',\n",
       " 'will',\n",
       " 'not',\n",
       " 'respond',\n",
       " 'to',\n",
       " 'any',\n",
       " 'new',\n",
       " 'found',\n",
       " 'miami',\n",
       " 'heat',\n",
       " 'fans']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_mountain_rows.iloc[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_mountain_rows.iloc[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_mountain_rows.to_csv(DATAGEN_CONFIG['PROCESSED_WNUT_PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-NERD\n",
    "Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens.\n",
    "\n",
    "https://huggingface.co/datasets/DFKI-SLT/few-nerd#dataset-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanbashtovyi/miniforge3/envs/ner_quant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds[\"train\"]\n",
    "ds_val = ds[\"validation\"]\n",
    "ds_test = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
      "    num_rows: 131767\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
      "    num_rows: 18824\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
      "    num_rows: 37648\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_train)\n",
    "print(ds_val)\n",
    "print(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'tokens': ['It',\n",
       "  'starred',\n",
       "  'Hicks',\n",
       "  \"'s\",\n",
       "  'wife',\n",
       "  ',',\n",
       "  'Ellaline',\n",
       "  'Terriss',\n",
       "  'and',\n",
       "  'Edmund',\n",
       "  'Payne',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 7, 0, 0, 0, 7, 7, 0, 7, 7, 0],\n",
       " 'fine_ner_tags': [0, 0, 51, 0, 0, 0, 50, 50, 0, 50, 50, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product'], id=None), length=-1, id=None),\n",
       " 'fine_ner_tags': Sequence(feature=ClassLabel(names=['O', 'art-broadcastprogram', 'art-film', 'art-music', 'art-other', 'art-painting', 'art-writtenart', 'building-airport', 'building-hospital', 'building-hotel', 'building-library', 'building-other', 'building-restaurant', 'building-sportsfacility', 'building-theater', 'event-attack/battle/war/militaryconflict', 'event-disaster', 'event-election', 'event-other', 'event-protest', 'event-sportsevent', 'location-GPE', 'location-bodiesofwater', 'location-island', 'location-mountain', 'location-other', 'location-park', 'location-road/railway/highway/transit', 'organization-company', 'organization-education', 'organization-government/governmentagency', 'organization-media/newspaper', 'organization-other', 'organization-politicalparty', 'organization-religion', 'organization-showorganization', 'organization-sportsleague', 'organization-sportsteam', 'other-astronomything', 'other-award', 'other-biologything', 'other-chemicalthing', 'other-currency', 'other-disease', 'other-educationaldegree', 'other-god', 'other-language', 'other-law', 'other-livingthing', 'other-medical', 'person-actor', 'person-artist/author', 'person-athlete', 'person-director', 'person-other', 'person-politician', 'person-scholar', 'person-soldier', 'product-airplane', 'product-car', 'product-food', 'product-game', 'product-other', 'product-ship', 'product-software', 'product-train', 'product-weapon'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the 'location-mountain' label, which might be useful to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUNTAIN_INDEX = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mountain_stats(dataset, mountain_tag=[MOUNTAIN_INDEX]):\n",
    "    \"\"\"\n",
    "    Displays statistics related to mountain tags in the dataset.\n",
    "    :param dataset: The dataset to analyze\n",
    "    :param mountain_tag: The tag representing mountains in the dataset\n",
    "    \"\"\"\n",
    "    mountain_count = sum(\n",
    "        tag in mountain_tag for line in tqdm(dataset) for tag in line['fine_ner_tags']\n",
    "    )\n",
    "\n",
    "    unique_mountains = {\n",
    "        line['tokens'][i] for line in dataset for i, tag in enumerate(line['fine_ner_tags'])\n",
    "        if tag in mountain_tag\n",
    "    }\n",
    "\n",
    "    samples_with_mountains = sum(\n",
    "        any(tag in mountain_tag for tag in line['fine_ner_tags']) for line in dataset\n",
    "    )\n",
    "\n",
    "    print(f\"Number of mountains = {mountain_count}\")\n",
    "    print(f\"Number of distinct mountains = {len(unique_mountains)}\")\n",
    "    print(f\"Number of samples with mountains = {samples_with_mountains}\")\n",
    "\n",
    "    stats = []\n",
    "    stats.append(mountain_count)\n",
    "    stats.append(len(unique_mountains))\n",
    "    stats.append(samples_with_mountains)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131767/131767 [00:04<00:00, 26934.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mountains = 4500\n",
      "Number of distinct mountains = 1871\n",
      "Number of samples with mountains = 1502\n",
      "##############################\n",
      "Val set stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18824/18824 [00:00<00:00, 25726.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mountains = 734\n",
      "Number of distinct mountains = 474\n",
      "Number of samples with mountains = 218\n",
      "##############################\n",
      "Test set stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37648/37648 [00:01<00:00, 24587.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mountains = 1366\n",
      "Number of distinct mountains = 776\n",
      "Number of samples with mountains = 448\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "print('Train set stats:')\n",
    "train_stats=display_mountain_stats(ds_train)\n",
    "print('###' * 10)\n",
    "\n",
    "print('Val set stats:')\n",
    "val_stats=display_mountain_stats(ds_val)\n",
    "print('###' * 10)\n",
    "\n",
    "print('Test set stats:')\n",
    "test_stats=display_mountain_stats(ds_test)\n",
    "print('###' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll map B-Mountain to an integer (e.g., 1), I-Mountain to another integer (e.g., 2), and O to 0\n",
    "#  in line with how ClassLabel expects integer labels.\n",
    "\n",
    "def mapping_function(example):\n",
    "    \"\"\"\n",
    "    A helper function that changes the tags like this:\n",
    "    Tokens labeled with 24 will be transformed to B-Mountain (1) or I-Mountain (2) based on their position.\n",
    "    All other tokens will remain the same.\n",
    "    :param example: a dataset sample\n",
    "    :return: the example with tags modified\n",
    "    \"\"\"\n",
    "    old_tags = example[\"fine_ner_tags\"]\n",
    "    new_tags = []\n",
    "\n",
    "    found_mountain = False  # Flag to track if a mountain label (24) is found\n",
    "\n",
    "    for tag in old_tags:\n",
    "        if tag == 24:\n",
    "            if not found_mountain:  # First occurrence of a mountain (B-Mountain)\n",
    "                new_tags.append(1)  # B-Mountain -> 1\n",
    "                found_mountain = True\n",
    "            else:  # Subsequent tokens of the same mountain (I-Mountain)\n",
    "                new_tags.append(2)  # I-Mountain -> 2\n",
    "        else:\n",
    "            new_tags.append(0)  # 'O' for all other tokens (unchanged)\n",
    "            found_mountain = False  # Reset the flag when it's not a mountain token\n",
    "\n",
    "    example['fine_ner_tags'] = new_tags\n",
    "    return example\n",
    "\n",
    "def modify_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Change the tags for every sample in the dataset by applying BIO labeling for mountains.\n",
    "    :param dataset: the dataset\n",
    "    :return: a modified dataset\n",
    "    \"\"\"\n",
    "    # Apply the mapping function to each sample in the dataset\n",
    "    dataset = dataset.map(mapping_function, batched=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_relabeled = modify_dataset(ds_train)\n",
    "val_relabeled = modify_dataset(ds_val)\n",
    "test_relabeled = modify_dataset(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataset, p):\n",
    "    \"\"\"\n",
    "    Reduce the dataset by keeping only a fraction of the samples that do not contain mountains.\n",
    "    If a sample contains 'B-Mountain' or 'I-Mountain', it will always be retained.\n",
    "    \n",
    "    :param dataset: the dataset\n",
    "    :param p: the fraction, 0<=p<=1\n",
    "    :return: the reduced dataset\n",
    "    \"\"\"\n",
    "    def keep_or_discard(example):\n",
    "        \"\"\"\n",
    "        Helper function to decide whether to keep or discard a sample.\n",
    "        :param example: the sample\n",
    "        :return: True if keeping the sample, False otherwise\n",
    "        \"\"\"\n",
    "        mountain_tags = {1, 2}\n",
    "        has_mountain = any(tag in mountain_tags for tag in example['fine_ner_tags'])\n",
    "        return has_mountain or np.random.rand() < p\n",
    "\n",
    "    return dataset.filter(keep_or_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train = balance_dataset(train_relabeled, train_stats[-1] / (ds_train.shape[0] - train_stats[-1]))\n",
    "balanced_val = balance_dataset(val_relabeled, val_stats[-1] / (ds_val.shape[0] - val_stats[-1]))\n",
    "balanced_test = balance_dataset(test_relabeled, test_stats[-1] / (ds_test.shape[0] - test_stats[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 3012\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3012/3012 [00:00<00:00, 9626.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mountains = 4500\n",
      "Number of distinct mountains = 1871\n",
      "Number of samples with mountains = 1502\n",
      "##############################\n",
      "Number of samples without mountains = 1510\n",
      "******************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:00<00:00, 8445.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mountains = 734\n",
      "Number of distinct mountains = 474\n",
      "Number of samples with mountains = 218\n",
      "##############################\n",
      "Number of samples without mountains = 228\n",
      "******************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 934/934 [00:00<00:00, 8768.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mountains = 1366\n",
      "Number of distinct mountains = 776\n",
      "Number of samples with mountains = 448\n",
      "##############################\n",
      "Number of samples without mountains = 486\n",
      "******************************\n",
      " \n"
     ]
    }
   ],
   "source": [
    "balanced_train_stats = display_mountain_stats(balanced_train, mountain_tag=[1, 2])\n",
    "print(\"###\" * 10)\n",
    "print(f\"Number of samples without mountains = {balanced_train.shape[0] - balanced_train_stats[-1]}\")\n",
    "print(\"***\" * 10)\n",
    "print(\" \")\n",
    "\n",
    "balanced_val_stats = display_mountain_stats(balanced_val, mountain_tag=[1, 2])\n",
    "print(\"###\" * 10)\n",
    "print(f\"Number of samples without mountains = {balanced_val.shape[0] - balanced_val_stats[-1]}\")\n",
    "print(\"***\" * 10)\n",
    "print(\" \")\n",
    "\n",
    "balanced_test_stats = display_mountain_stats(balanced_test, mountain_tag=[1, 2])\n",
    "print(\"###\" * 10)\n",
    "print(f\"Number of samples without mountains = {balanced_test.shape[0] - balanced_test_stats[-1]}\")\n",
    "print(\"***\" * 10)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3012/3012 [00:00<00:00, 72292.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 446/446 [00:00<00:00, 84496.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 934/934 [00:00<00:00, 136274.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "balanced_train.save_to_disk(DATAGEN_CONFIG['FEW-NERD_BALANCED_TRAIN_PATH'])\n",
    "balanced_val.save_to_disk(DATAGEN_CONFIG['FEW-NERD_BALANCED_VAL_PATH'])\n",
    "balanced_test.save_to_disk(DATAGEN_CONFIG['FEW-NERD_BALANCED_TEST_PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resulting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4784, 3)\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Process the synthetic dataset\n",
    "############################\n",
    "with open(DATAGEN_CONFIG['SAVE_PROCESSED_PATH'], 'r') as f:\n",
    "    synthetic_data = json.load(f)\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "synthetic_df['tokens'] = synthetic_df['tokens'].apply(lambda x: [str(token) for token in x])\n",
    "synthetic_df['labels'] = synthetic_df['labels'].apply(lambda x: [int(label) for label in x])\n",
    "\n",
    "############################\n",
    "# Process the WNUT dataset\n",
    "############################\n",
    "wnut_data = pd.read_csv(DATAGEN_CONFIG['PROCESSED_WNUT_PATH'])\n",
    "wnut_data['tokens'] = wnut_data['tokens'].apply(lambda x: eval(x))  # Convert string to list\n",
    "wnut_data['labels'] = wnut_data['labels'].apply(lambda x: eval(x))\n",
    "\n",
    "wnut_df = pd.DataFrame({\n",
    "    'sentence': wnut_data['tokens'].apply(lambda x: ' '.join(x)),\n",
    "    'tokens': wnut_data['tokens'],\n",
    "    'labels': wnut_data['labels']\n",
    "})\n",
    "\n",
    "wnut_df['tokens'] = wnut_df['tokens'].apply(lambda x: [str(token) for token in x])\n",
    "wnut_df['labels'] = wnut_df['labels'].apply(lambda x: [int(label) for label in x])\n",
    "\n",
    "############################\n",
    "# Combine synthetic & WNUT data\n",
    "############################\n",
    "combined_data = pd.concat([synthetic_df, wnut_df], ignore_index=True)\n",
    "combined_dataset = Dataset.from_pandas(combined_data)\n",
    "\n",
    "############################\n",
    "# Process the Few-NERD datasets (train, val, test)\n",
    "############################\n",
    "few_nerd_train = load_from_disk(DATAGEN_CONFIG['FEW-NERD_BALANCED_TRAIN_PATH'])\n",
    "few_nerd_val = load_from_disk(DATAGEN_CONFIG['FEW-NERD_BALANCED_VAL_PATH'])\n",
    "few_nerd_test = load_from_disk(DATAGEN_CONFIG['FEW-NERD_BALANCED_TEST_PATH'])\n",
    "\n",
    "# Function to convert Few-NERD dataset to DataFrame\n",
    "def process_few_nerd(few_nerd_data):\n",
    "    few_nerd_df = pd.DataFrame({\n",
    "        'sentence': [\" \".join(tokens) for tokens in few_nerd_data['tokens']],\n",
    "        'tokens': few_nerd_data['tokens'],\n",
    "        'labels': few_nerd_data['fine_ner_tags']\n",
    "    })\n",
    "    few_nerd_df['tokens'] = few_nerd_df['tokens'].apply(lambda x: [str(token) for token in x])\n",
    "    few_nerd_df['labels'] = few_nerd_df['labels'].apply(lambda x: [int(label) for label in x])\n",
    "    return Dataset.from_pandas(few_nerd_df)\n",
    "\n",
    "# Process Few-NERD train, val, and test\n",
    "few_nerd_train_dataset = process_few_nerd(few_nerd_train)\n",
    "few_nerd_val_dataset = process_few_nerd(few_nerd_val)\n",
    "few_nerd_test_dataset = process_few_nerd(few_nerd_test)\n",
    "\n",
    "############################\n",
    "# Combine all datasets\n",
    "############################\n",
    "final_dataset = concatenate_datasets([combined_dataset, few_nerd_train_dataset, few_nerd_val_dataset, few_nerd_test_dataset])\n",
    "shuffled_dataset = final_dataset.shuffle(seed=42)\n",
    "print(shuffled_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Split into train, val, and test sets\n",
    "############################\n",
    "train_size = int(0.8 * len(shuffled_dataset))\n",
    "val_size = int(0.1 * len(shuffled_dataset))\n",
    "\n",
    "train_dataset = shuffled_dataset.select(range(train_size))\n",
    "val_dataset = shuffled_dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = shuffled_dataset.select(range(train_size + val_size, len(shuffled_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "console :\n",
    "\n",
    "     huggingface-cli login\n",
    "      \n",
    "notebook : \n",
    "    \n",
    "    from huggingface_hub import notebook_login\n",
    "    \n",
    "    notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 43.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 59.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 53.24ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/telord/mountains-ner-dataset/commit/a086868ec3fb3336574e24284c74e19e127e9d4d', commit_message='Upload dataset', commit_description='', oid='a086868ec3fb3336574e24284c74e19e127e9d4d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/telord/mountains-ner-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='telord/mountains-ner-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# Push datasets to Hugging Face Hub\n",
    "############################\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'val': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\"telord/mountains-ner-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Virender Sehwag and Harbhajan Singh were not named in the Indian squad due to recent poor performances while Irfan Pathan and Munaf Patel were not fully fit .',\n",
       " 'tokens': ['Virender',\n",
       "  'Sehwag',\n",
       "  'and',\n",
       "  'Harbhajan',\n",
       "  'Singh',\n",
       "  'were',\n",
       "  'not',\n",
       "  'named',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Indian',\n",
       "  'squad',\n",
       "  'due',\n",
       "  'to',\n",
       "  'recent',\n",
       "  'poor',\n",
       "  'performances',\n",
       "  'while',\n",
       "  'Irfan',\n",
       "  'Pathan',\n",
       "  'and',\n",
       "  'Munaf',\n",
       "  'Patel',\n",
       "  'were',\n",
       "  'not',\n",
       "  'fully',\n",
       "  'fit',\n",
       "  '.'],\n",
       " 'labels': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The Sand Springs Range was the site of Project Shoal , an underground nuclear test conducted as part of the Vela Uniform program .',\n",
       " 'tokens': ['The',\n",
       "  'Sand',\n",
       "  'Springs',\n",
       "  'Range',\n",
       "  'was',\n",
       "  'the',\n",
       "  'site',\n",
       "  'of',\n",
       "  'Project',\n",
       "  'Shoal',\n",
       "  ',',\n",
       "  'an',\n",
       "  'underground',\n",
       "  'nuclear',\n",
       "  'test',\n",
       "  'conducted',\n",
       "  'as',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Vela',\n",
       "  'Uniform',\n",
       "  'program',\n",
       "  '.'],\n",
       " 'labels': [0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset[321]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Not much is known about him .',\n",
       " 'tokens': ['Not', 'much', 'is', 'known', 'about', 'him', '.'],\n",
       " 'labels': [0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset[2200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
